{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f510d5cd-3437-4e33-9b85-d7204ec4101f",
   "metadata": {},
   "source": [
    "# This notebook creates a consolidated and curated 5-year world happiness data from year 2020 to 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba878ba-ba34-4a05-8bfc-d8c6229062a3",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a093a9d-a1ca-46ca-9f95-a9575c0fa143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb652486-74fc-42fa-a239-dbf45fa7cc6f",
   "metadata": {},
   "source": [
    "# Sourcing: Download raw data and convert to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57da9663-f19d-4bc3-9d6e-67c4c166c12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: srs_file_2020.xls\n",
      "Converted to CSV: srs_file_2020.csv\n",
      "Downloaded: srs_file_2021.xls\n",
      "Converted to CSV: srs_file_2021.csv\n",
      "Downloaded: srs_file_2022.xls\n",
      "Converted to CSV: srs_file_2022.csv\n",
      "Downloaded: srs_file_2023.xls\n",
      "Converted to CSV: srs_file_2023.csv\n",
      "Downloaded: srs_file_2024.xls\n",
      "Converted to CSV: srs_file_2024.csv\n",
      "Updated srs_file_2020.csv with YEAR column and removed empty Ladder score rows.\n",
      "Updated srs_file_2021.csv with YEAR column and removed empty Ladder score rows.\n",
      "Updated srs_file_2022.csv with YEAR column and removed empty Ladder score rows.\n",
      "Updated srs_file_2023.csv with YEAR column and removed empty Ladder score rows.\n",
      "Updated srs_file_2024.csv with YEAR column and removed empty Ladder score rows.\n"
     ]
    }
   ],
   "source": [
    "# List of (URL, new file name) pairs\n",
    "files_to_download = [\n",
    "    (\"https://happiness-report.s3.amazonaws.com/2020/WHR20_DataForFigure2.1.xls\", \"srs_file_2020.xls\"),\n",
    "    (\"https://happiness-report.s3.amazonaws.com/2021/DataForFigure2.1WHR2021C2.xls\", \"srs_file_2021.xls\"),\n",
    "    (\"https://happiness-report.s3.amazonaws.com/2022/Appendix_2_Data_for_Figure_2.1.xls\", \"srs_file_2022.xls\"),\n",
    "    (\"https://happiness-report.s3.amazonaws.com/2023/DataForFigure2.1WHR2023.xls\", \"srs_file_2023.xls\"),\n",
    "    (\"https://happiness-report.s3.amazonaws.com/2024/DataForFigure2.1+with+sub+bars+2024.xls\", \"srs_file_2024.xls\"),\n",
    "]\n",
    "\n",
    "# Function to download files and convert to CSV\n",
    "def download_and_convert_to_csv(file_list):\n",
    "    for url, excel_filename in file_list:\n",
    "        # Step 1: Download the Excel file\n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(excel_filename, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "        print(f\"Downloaded: {excel_filename}\")\n",
    "        \n",
    "        # Step 2: Convert to CSV\n",
    "        csv_filename = excel_filename.replace(\".xls\", \".csv\").replace(\".xlsx\", \".csv\")\n",
    "        \n",
    "        # Automatically select the correct engine\n",
    "        if excel_filename.endswith(\".xls\"):\n",
    "            engine = \"xlrd\"  # Old Excel format\n",
    "        else:\n",
    "            engine = \"openpyxl\"  # New Excel format\n",
    "        \n",
    "        # Load and convert to CSV\n",
    "        df = pd.read_excel(excel_filename, engine=engine)\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "        \n",
    "        print(f\"Converted to CSV: {csv_filename}\")\n",
    "\n",
    "# Run the function\n",
    "download_and_convert_to_csv(files_to_download)\n",
    "\n",
    "# List of (file name, year) pairs\n",
    "files_with_years = [\n",
    "    (\"srs_file_2020.csv\", 2020),\n",
    "    (\"srs_file_2021.csv\", 2021),\n",
    "    (\"srs_file_2022.csv\", 2022),\n",
    "    (\"srs_file_2023.csv\", 2023),\n",
    "    (\"srs_file_2024.csv\", 2024),\n",
    "]\n",
    "\n",
    "# Process each file\n",
    "for file_name, year in files_with_years:\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_name)\n",
    "    \n",
    "    # Add YEAR column\n",
    "    df[\"Year\"] = year\n",
    "    \n",
    "    # Remove rows where either 'Ladder score' or 'Happiness score' is empty\n",
    "    if \"Ladder score\" in df.columns:\n",
    "        df = df.dropna(subset=[\"Ladder score\"])\n",
    "    elif \"Happiness score\" in df.columns:\n",
    "        df = df.dropna(subset=[\"Happiness score\"])\n",
    "\n",
    "    # Save the updated dataset\n",
    "    df.to_csv(file_name, index=False)\n",
    "    \n",
    "    print(f\"Updated {file_name} with YEAR column and removed empty Ladder score rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad0f1383-e044-40db-ba27-326ae440b291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Western Europe', 'North America and ANZ',\n",
       "       'Middle East and North Africa', 'Latin America and Caribbean',\n",
       "       'Central and Eastern Europe', 'East Asia', 'Southeast Asia',\n",
       "       'Commonwealth of Independent States', 'Sub-Saharan Africa',\n",
       "       'South Asia'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"srs_file_2021.csv\")\n",
    "df[\"Regional indicator\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6ed28a-5b95-45b1-bbff-27315f965cad",
   "metadata": {},
   "source": [
    "# Staging: Select only the required columns, standardise coloumn names, and add in regional mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12082c4d-209e-47e7-bc4c-d202542fa8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned file saved as stg_file_2020.csv\n",
      "Cleaned file saved as stg_file_2021.csv\n",
      "Cleaned file saved as stg_file_2022.csv\n",
      "Cleaned file saved as stg_file_2023.csv\n",
      "Cleaned file saved as stg_file_2024.csv\n"
     ]
    }
   ],
   "source": [
    "# Define standard column mappings for consistency\n",
    "COLUMN_MAPPING = {\n",
    "    'Country name': 'Country',\n",
    "    'Country': 'Country',\n",
    "    'Ladder score': 'Ladder Score',\n",
    "    'Happiness score': 'Ladder Score',  # 2022 uses this\n",
    "    'Logged GDP per capita': 'GDP per Capita',\n",
    "    'Explained by: Log GDP per capita': 'GDP per Capita',  # 2024\n",
    "    'Explained by: GDP per capita': 'GDP per Capita',  # 2022\n",
    "    'Social support': 'Social Support',\n",
    "    'Explained by: Social support': 'Social Support',  # 2022, 2024\n",
    "    'Healthy life expectancy': 'Healthy Life Expectancy',\n",
    "    'Explained by: Healthy life expectancy': 'Healthy Life Expectancy',  # 2022, 2024\n",
    "    'Freedom to make life choices': 'Freedom to Make Life Choices',\n",
    "    'Explained by: Freedom to make life choices': 'Freedom to Make Life Choices',  # 2022, 2024\n",
    "    'Generosity': 'Generosity',\n",
    "    'Explained by: Generosity': 'Generosity',  # 2022, 2024\n",
    "    'Perceptions of corruption': 'Perceptions of Corruption',\n",
    "    'Explained by: Perceptions of corruption': 'Perceptions of Corruption',  # 2022, 2024\n",
    "    'Year': 'Year'\n",
    "}\n",
    "\n",
    "# Define the essential columns to retain\n",
    "REQUIRED_COLUMNS = ['Year', 'Country', 'Ladder Score', 'GDP per Capita', \n",
    "                    'Social Support', 'Healthy Life Expectancy', 'Freedom to Make Life Choices', \n",
    "                    'Generosity', 'Perceptions of Corruption']\n",
    "\n",
    "# Mapping of dataset filenames to corresponding years\n",
    "files_with_years = {\n",
    "    2020: \"srs_file_2020.csv\",\n",
    "    2021: \"srs_file_2021.csv\",\n",
    "    2022: \"srs_file_2022.csv\",\n",
    "    2023: \"srs_file_2023.csv\",\n",
    "    2024: \"srs_file_2024.csv\"\n",
    "}\n",
    "\n",
    "# Function to clean all string columns by removing leading/trailing spaces and trailing '*'\n",
    "def clean_dataframe(df):\n",
    "    str_cols = df.select_dtypes(include=[\"object\"]).columns  # Select only string columns\n",
    "    df[str_cols] = df[str_cols].map(lambda x: x.strip().rstrip('*') if isinstance(x, str) else x)\n",
    "    return df\n",
    "\n",
    "# Step 1: Process datasets\n",
    "# region_mapping = {}\n",
    "for year, file_name in files_with_years.items():\n",
    "    if not os.path.exists(file_name):\n",
    "        print(f\"File {file_name} not found, skipping...\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(file_name).rename(columns=COLUMN_MAPPING)\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    df = clean_dataframe(df)\n",
    "    df = df[[col for col in REQUIRED_COLUMNS if col in df.columns]]\n",
    "    df = df.dropna(subset=REQUIRED_COLUMNS)  # Drop rows with missing required data\n",
    "\n",
    "    cleaned_file = f\"stg_file_{year}.csv\"\n",
    "    df.to_csv(cleaned_file, index=False)\n",
    "    print(f\"Cleaned file saved as {cleaned_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33c1ca17-87d5-41f9-9817-a0090253dfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ No 'unknown' values found in stg_file_2020.csv.\n",
      "‚úÖ No 'unknown' values found in stg_file_2021.csv.\n",
      "‚úÖ No 'unknown' values found in stg_file_2022.csv.\n",
      "‚úÖ No 'unknown' values found in stg_file_2023.csv.\n",
      "‚úÖ No 'unknown' values found in stg_file_2024.csv.\n",
      "\n",
      "üîç Check for 'unknown' values completed!\n"
     ]
    }
   ],
   "source": [
    "# List of staging files to check\n",
    "staging_files = [\n",
    "    \"stg_file_2020.csv\",\n",
    "    \"stg_file_2021.csv\",\n",
    "    \"stg_file_2022.csv\",\n",
    "    \"stg_file_2023.csv\",\n",
    "    \"stg_file_2024.csv\"\n",
    "]\n",
    "\n",
    "# Function to check for 'unknown' values in any column\n",
    "def find_unknown_values(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    unknown_rows = df[df.apply(lambda row: row.astype(str).str.contains(\"unknown\", case=False, na=False).any(), axis=1)]\n",
    "    return unknown_rows\n",
    "\n",
    "# Iterate over each staging file and check for 'unknown' values\n",
    "for file in staging_files:\n",
    "    if os.path.exists(file):\n",
    "        unknown_rows = find_unknown_values(file)\n",
    "        if not unknown_rows.empty:\n",
    "            print(f\"\\n‚ö†Ô∏è Rows with 'unknown' values found in {file}:\")\n",
    "            print(unknown_rows.iloc[:, :4])\n",
    "        else:\n",
    "            print(f\"‚úÖ No 'unknown' values found in {file}.\")\n",
    "    else:\n",
    "        print(f\"‚ùå File {file} not found, skipping...\")\n",
    "\n",
    "print(\"\\nüîç Check for 'unknown' values completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b0316c-3546-4e78-a6b6-1d7b3d768fd8",
   "metadata": {},
   "source": [
    "# Integrate: Combine the 5 datasets, add region & continent mapping, and reoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43f87acb-31ed-43a2-b7b1-4826711102d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Combined dataset saved as int_happiness_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "int_dataset_name = \"int_happiness_dataset.csv\"\n",
    "continent_region_mapping_file = \"country_to_continent.csv\"\n",
    "\n",
    "# List of cleaned dataset files\n",
    "merged_files = [f\"stg_file_{year}.csv\" for year in range(2020, 2025)]\n",
    "\n",
    "# Load and combine all datasets\n",
    "dataframes = [pd.read_csv(file) for file in merged_files if os.path.exists(file)]\n",
    "if not dataframes:\n",
    "    raise ValueError(\"No valid dataset files found. Please check file paths.\")\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Load country-to-region mapping dataset\n",
    "if not os.path.exists(continent_region_mapping_file):\n",
    "    raise FileNotFoundError(f\"Mapping file '{continent_region_mapping_file}' not found.\")\n",
    "continent_region_mapping_df = pd.read_csv(continent_region_mapping_file)\n",
    "\n",
    "# Create mapping dictionaries\n",
    "country_to_region = dict(zip(continent_region_mapping_df[\"name\"], continent_region_mapping_df[\"sub-region\"]))\n",
    "region_to_continent = dict(zip(continent_region_mapping_df[\"sub-region\"], continent_region_mapping_df[\"region\"]))\n",
    "\n",
    "# Ensure \"Region\" and \"Continent\" columns exist before transformation\n",
    "merged_df[\"Region\"] = merged_df.get(\"Region\", \"Unknown\")\n",
    "merged_df[\"Continent\"] = merged_df.get(\"Continent\", \"Unknown\")\n",
    "\n",
    "# Rename and standardise countries\n",
    "merged_df[\"Country\"] = merged_df[\"Country\"].replace({\n",
    "    \"Taiwan Province of China\": \"Taiwan\",\n",
    "    \"Eswatini, Kingdom of\": \"Eswatini\",\n",
    "    \"Turkiye\": \"Turkey\",\n",
    "    \"Congo\": \"Congo (Brazzaville)\"\n",
    "})\n",
    "\n",
    "# Fill missing \"Region\" values using country mapping\n",
    "merged_df[\"Region\"] = merged_df[\"Country\"].map(country_to_region).fillna(merged_df[\"Region\"])\n",
    "\n",
    "# Fill missing \"Continent\" values using region mapping\n",
    "merged_df[\"Continent\"] = merged_df[\"Region\"].map(region_to_continent).fillna(merged_df[\"Continent\"])\n",
    "\n",
    "# Manually define corrections for missing Region & Continent\n",
    "manual_updates = {\n",
    "    \"Bosnia and Herzegovina\": {\"Region\": \"Southern Europe\", \"Continent\": \"Europe\"},\n",
    "    \"Congo (Brazzaville)\": {\"Region\": \"Sub-Saharan Africa\", \"Continent\": \"Africa\"},\n",
    "    \"Congo (Kinshasa)\": {\"Region\": \"Sub-Saharan Africa\", \"Continent\": \"Africa\"},\n",
    "    \"Hong Kong S.A.R. of China\": {\"Region\": \"Eastern Asia\", \"Continent\": \"Asia\"},\n",
    "    \"Ivory Coast\": {\"Region\": \"Sub-Saharan Africa\", \"Continent\": \"Africa\"},\n",
    "    \"Kosovo\": {\"Region\": \"Southern Europe\", \"Continent\": \"Europe\"},\n",
    "    \"North Cyprus\": {\"Region\": \"Western Asia\", \"Continent\": \"Asia\"},\n",
    "    \"North Macedonia\": {\"Region\": \"Southern Europe\", \"Continent\": \"Europe\"},\n",
    "    \"Palestinian Territories\": {\"Region\": \"Western Asia\", \"Continent\": \"Asia\"},\n",
    "    \"Swaziland\": {\"Region\": \"Sub-Saharan Africa\", \"Continent\": \"Africa\"},\n",
    "    \"Czechia\": {\"Region\": \"Central Europe\", \"Continent\": \"Europe\"},\n",
    "}\n",
    "\n",
    "# Apply manual corrections\n",
    "for country, values in manual_updates.items():\n",
    "    merged_df.loc[merged_df[\"Country\"] == country, [\"Region\", \"Continent\"]] = values[\"Region\"], values[\"Continent\"]\n",
    "\n",
    "# Sort and arrange columns for better readability\n",
    "merged_df = merged_df.sort_values(by=[\"Year\", \"Continent\", \"Country\"])[[\n",
    "    \"Year\", \"Country\", \"Region\", \"Continent\", \"Ladder Score\", \"GDP per Capita\", \n",
    "    \"Social Support\", \"Healthy Life Expectancy\", \"Freedom to Make Life Choices\", \n",
    "    \"Generosity\", \"Perceptions of Corruption\"\n",
    "]]\n",
    "\n",
    "# Save the final dataset\n",
    "merged_df.to_csv(int_dataset_name, index=False)\n",
    "print(f\"‚úÖ Combined dataset saved as {int_dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69417d80-7019-445e-ad16-833a84ba7ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Continent                           Region\n",
      "0     Africa                  Northern Africa\n",
      "1     Africa               Sub-Saharan Africa\n",
      "2   Americas  Latin America and the Caribbean\n",
      "3   Americas                 Northern America\n",
      "4       Asia                     Central Asia\n",
      "5       Asia                     Eastern Asia\n",
      "6       Asia               South-eastern Asia\n",
      "7       Asia                    Southern Asia\n",
      "8       Asia                     Western Asia\n",
      "9     Europe                   Central Europe\n",
      "10    Europe                   Eastern Europe\n",
      "11    Europe                  Northern Europe\n",
      "12    Europe                  Southern Europe\n",
      "13    Europe                   Western Europe\n",
      "14   Oceania        Australia and New Zealand\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"int_happiness_dataset.csv\")\n",
    "# Assuming df is your DataFrame\n",
    "unique_combinations = df.groupby([\"Continent\", \"Region\"]).size().reset_index()[[\"Continent\", \"Region\"]]\n",
    "\n",
    "print(unique_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63fa6b4f-a8da-4188-95a1-9214d8ea1a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ No 'unknown' values found in int_happiness_dataset.csv.\n",
      "\n",
      "üîç Check for 'unknown' values completed!\n"
     ]
    }
   ],
   "source": [
    "def find_unknown_values(file_path):\n",
    "    \"\"\"\n",
    "    Checks for 'unknown' values (case-insensitive) in any column of a CSV file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing rows with 'unknown' values, or an empty DataFrame if none are found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        unknown_rows = df[df.apply(lambda row: row.astype(str).str.contains(\"unknown\", case=False, na=False).any(), axis=1)]\n",
    "        return unknown_rows\n",
    "    except FileNotFoundError:\n",
    "        return None # Return None when file not found.\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None # Return None in case of other errors.\n",
    "\n",
    "# File path\n",
    "file = \"int_happiness_dataset.csv\"\n",
    "\n",
    "# Check for 'unknown' values\n",
    "unknown_rows = find_unknown_values(file)\n",
    "\n",
    "if unknown_rows is None:\n",
    "    if not os.path.exists(file):\n",
    "        print(f\"‚ùå File {file} not found, skipping...\")\n",
    "    else:\n",
    "        print(f\"‚ùå An error occurred while processing {file}\")\n",
    "elif unknown_rows.empty:\n",
    "    print(f\"‚úÖ No 'unknown' values found in {file}.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Rows with 'unknown' values found in {file}:\")\n",
    "    print(unknown_rows.iloc[:, :4])\n",
    "\n",
    "print(\"\\nüîç Check for 'unknown' values completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccc1c6fa-0ebd-4f48-ad33-a2d9af18721b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'Region' column: ['Northern Africa' 'Sub-Saharan Africa' 'Latin America and the Caribbean'\n",
      " 'Northern America' 'Southern Asia' 'Western Asia' 'South-eastern Asia'\n",
      " 'Eastern Asia' 'Central Asia' 'Southern Europe' 'Western Europe'\n",
      " 'Eastern Europe' 'Northern Europe' 'Australia and New Zealand'\n",
      " 'Central Europe']\n"
     ]
    }
   ],
   "source": [
    "# Find unique values in a specific column (e.g., \"Region\")\n",
    "unique_values = merged_df[\"Region\"].unique()\n",
    "\n",
    "# Print unique values\n",
    "print(\"Unique values in 'Region' column:\", unique_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f4fabc-585c-45db-845c-907051ae546b",
   "metadata": {},
   "source": [
    "# Reporting: Curate the dataset to include KPIs such as average regional ladder score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2e8b9e4-f3ec-4fab-a74a-2254cf0c2fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset saved as 'reporting_happiness_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "int_dataset_path = \"int_happiness_dataset.csv\"  # Input dataset\n",
    "reporting_dataset_path = \"reporting_happiness_dataset.csv\"  # Output dataset\n",
    "\n",
    "# Load the merged dataset\n",
    "if not os.path.exists(int_dataset_path):\n",
    "    raise FileNotFoundError(f\"File '{int_dataset_path}' not found. Please check the file path.\")\n",
    "\n",
    "merged_df = pd.read_csv(int_dataset_path)\n",
    "\n",
    "# Convert relevant columns to numeric (handling errors by coercing non-numeric values to NaN)\n",
    "merged_df['Ladder Score'] = pd.to_numeric(merged_df['Ladder Score'], errors='coerce')\n",
    "merged_df['GDP per Capita'] = pd.to_numeric(merged_df['GDP per Capita'], errors='coerce')\n",
    "merged_df['Social Support'] = pd.to_numeric(merged_df['Social Support'], errors='coerce')\n",
    "merged_df['Healthy Life Expectancy'] = pd.to_numeric(merged_df['Healthy Life Expectancy'], errors='coerce')\n",
    "merged_df['Freedom to Make Life Choices'] = pd.to_numeric(merged_df['Freedom to Make Life Choices'], errors='coerce')\n",
    "merged_df['Generosity'] = pd.to_numeric(merged_df['Generosity'], errors='coerce')\n",
    "merged_df['Perceptions of Corruption'] = pd.to_numeric(merged_df['Perceptions of Corruption'], errors='coerce')\n",
    "\n",
    "# Calculate average regional Ladder Score and GDP per Capita\n",
    "regional_avg = merged_df.groupby(['Year', 'Region'])[['Ladder Score', 'GDP per Capita', 'Social Support', \n",
    "                                                      'Healthy Life Expectancy', 'Freedom to Make Life Choices', \n",
    "                                                      'Generosity', 'Perceptions of Corruption']].mean().reset_index()\n",
    "\n",
    "# Rename columns to match requirements\n",
    "regional_avg.rename(columns={\n",
    "    'Ladder Score': 'Average Regional Ladder Score',\n",
    "    'GDP per Capita': 'Average Regional GDP per Capita',\n",
    "    'Social Support': 'Average Regional Social Support',\n",
    "    'Healthy Life Expectancy': 'Average Regional Healthy Life Expectancy', \n",
    "    'Freedom to Make Life Choices': 'Average Regional Freedom to Make Life Choices', \n",
    "    'Generosity': 'Average Regional Generosity', \n",
    "    'Perceptions of Corruption': 'Average Regional Perceptions of Corruption'\n",
    "}, inplace=True)\n",
    "\n",
    "# Merge the calculated averages back into the original dataset\n",
    "merged_df = merged_df.merge(regional_avg, on=['Year', 'Region'], how='left')\n",
    "\n",
    "# Ensure correct column order\n",
    "merged_df = merged_df[['Year', 'Country', 'Region', 'Continent', \n",
    "                       'Ladder Score', 'Average Regional Ladder Score',\n",
    "                       'GDP per Capita', 'Average Regional GDP per Capita',\n",
    "                       'Social Support', 'Average Regional Social Support',\n",
    "                       'Healthy Life Expectancy', 'Average Regional Healthy Life Expectancy',\n",
    "                       'Freedom to Make Life Choices', 'Average Regional Freedom to Make Life Choices', \n",
    "                       'Generosity', 'Average Regional Generosity',\n",
    "                       'Perceptions of Corruption', 'Average Regional Perceptions of Corruption']]\n",
    "\n",
    "# Save the updated dataset\n",
    "merged_df.to_csv(reporting_dataset_path, index=False)\n",
    "\n",
    "print(f\"Updated dataset saved as '{reporting_dataset_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c87af0e1-b684-4cf8-8f6d-440d7adcc5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ No 'unknown' values found in reporting_happiness_dataset.csv.\n",
      "\n",
      "üîç Check for 'unknown' values completed!\n"
     ]
    }
   ],
   "source": [
    "def find_unknown_values(file_path):\n",
    "    \"\"\"\n",
    "    Checks for 'unknown' values (case-insensitive) in any column of a CSV file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing rows with 'unknown' values, or an empty DataFrame if none are found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        unknown_rows = df[df.apply(lambda row: row.astype(str).str.contains(\"unknown\", case=False, na=False).any(), axis=1)]\n",
    "        return unknown_rows\n",
    "    except FileNotFoundError:\n",
    "        return None # Return None when file not found.\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None # Return None in case of other errors.\n",
    "\n",
    "# File path\n",
    "file = \"reporting_happiness_dataset.csv\"\n",
    "\n",
    "# Check for 'unknown' values\n",
    "unknown_rows = find_unknown_values(file)\n",
    "\n",
    "if unknown_rows is None:\n",
    "    if not os.path.exists(file):\n",
    "        print(f\"‚ùå File {file} not found, skipping...\")\n",
    "    else:\n",
    "        print(f\"‚ùå An error occurred while processing {file}\")\n",
    "elif unknown_rows.empty:\n",
    "    print(f\"‚úÖ No 'unknown' values found in {file}.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Rows with 'unknown' values found in {file}:\")\n",
    "    print(unknown_rows.iloc[:, :4])\n",
    "\n",
    "print(\"\\nüîç Check for 'unknown' values completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:532]",
   "language": "python",
   "name": "conda-env-532-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
